import os
import shutil
import glob

# â”€â”€â”€â”€â”€â”€â”€â”€ CLEAN UP ANY STALE LOCKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for path in glob.glob("/cache/**/*.lock", recursive=True):
    try:
        if os.path.isdir(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
    except Exception:
        pass

# â”€â”€â”€ Make sure all HF/Transformers caches go to /cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.setdefault("HF_HOME", "/cache")
os.environ.setdefault("TRANSFORMERS_CACHE", "/cache")
os.environ.setdefault("HF_DATASETS_CACHE", "/cache")
os.environ.setdefault("HF_METRICS_CACHE", "/cache")

import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# â”€â”€â”€ Streamlit page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Groot Growth â€“ Financial Companion")
st.title("ðŸŒ± Groot Growth â€“ Your AI Financial Guide")

# â”€â”€â”€ Load your training corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open("training.txt", "r") as f:
    training_corpus = f.read()

# â”€â”€â”€ Initialize the generator (cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def init_generator():
    tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Coder-1.3B-base")
    model     = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-Coder-1.3B-base")
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

# â”€â”€â”€ Lazily load the model into session_state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "generator" not in st.session_state:
    with st.spinner("Loading Grootâ€™s brainâ€¦ this may take a minute"):
        st.session_state.generator = init_generator()

# â”€â”€â”€ User input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_prompt = st.text_area("ðŸ’¬ Ask Groot about finances (saving, credit, investing):")

# â”€â”€â”€ On-click generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("Grow with Groot ðŸŒ³"):
    if not user_prompt.strip():
        st.warning("Please enter a question for Groot!")
    else:
        with st.spinner("Groot is thinkingâ€¦ ðŸŒ¿"):
            # 1) System instruction + corpus
            system_preface = (
                "Groot is an expert AI financial guide. "
                "He answers clearly, concisely, and in full sentences.\n"
            )
            full_prompt = (
                system_preface
                + training_corpus
                + f"\nUser: {user_prompt}\nGroot:"
            )

            # 2) Generate with greedy decoding for consistency
            result = st.session_state.generator(
                full_prompt,
                max_new_tokens=100,
                do_sample=False,
                temperature=0.0,
            )

        # 3) Strip out only Grootâ€™s reply
        raw = result[0]["generated_text"]
        answer = raw.split("Groot:")[-1].split("User:")[0].strip()

        st.markdown("### ðŸŒ¿ Groot Says:")
        st.write(answer)
